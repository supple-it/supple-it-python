{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#추천시스템1\n",
    "\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40459, 482)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "from scipy.sparse import vstack\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# CSV 파일 로드\n",
    "data = pd.read_csv('../data/supplements_20250311.csv')  # your_data.csv에 데이터 로드 (가정: 텍스트 컬럼이 '기능성'이라고 가정)\n",
    "\n",
    "# KoNLPy의 Okt 객체를 사용하여 형태소 분석\n",
    "okt = Okt()\n",
    "\n",
    "# 불용어 목록을 텍스트 파일에서 읽어오기\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords = file.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "# 불용어 리스트 로드\n",
    "stop_words = load_stopwords('../data/stopwords-ko.txt')\n",
    "\n",
    "# 텍스트 전처리 함수 (형태소 분석 및 불용어 처리)\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):  # NaN 값이 있을 경우 빈 문자열로 처리\n",
    "        return ''\n",
    "    try:\n",
    "        # 형태소 분석을 통해 명사만 추출\n",
    "        nouns = okt.nouns(text)\n",
    "        # 불용어 제거\n",
    "        filtered_nouns = [word for word in nouns if word not in stop_words]\n",
    "        return ' '.join(filtered_nouns) if filtered_nouns else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:30]}... -> {e}\")  # 텍스트 앞 30글자만 출력\n",
    "        return ''\n",
    "\n",
    "# 텍스트 전처리\n",
    "data['processed_text'] = data['기능성'].apply(preprocess_text)\n",
    "\n",
    "# TF-IDF 벡터화 함수\n",
    "def process_tfidf_batch(batch_data, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer()  # 불용어 처리 이미 완료된 데이터 사용\n",
    "        tfidf_matrix = vectorizer.fit_transform(batch_data)  # TF-IDF 매트릭스 생성\n",
    "    else:\n",
    "        tfidf_matrix = vectorizer.transform(batch_data)  # 기존 벡터라이저를 사용하여 변환\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# 배치 단위로 처리하기\n",
    "batch_size = 5000  # 5000개씩 처리 (배치 크기 조정 가능)\n",
    "tfidf_results = []\n",
    "\n",
    "# 첫 번째 배치에서 벡터라이저 학습\n",
    "first_batch = data['processed_text'][:batch_size]\n",
    "tfidf_matrix, tfidf_vectorizer = process_tfidf_batch(first_batch)\n",
    "\n",
    "tfidf_results.append(tfidf_matrix)\n",
    "\n",
    "# 나머지 배치 처리\n",
    "for start in range(batch_size, len(data), batch_size):\n",
    "    end = min(start + batch_size, len(data))\n",
    "    batch_data = data['processed_text'][start:end]\n",
    "    \n",
    "    tfidf_batch, _ = process_tfidf_batch(batch_data, tfidf_vectorizer)\n",
    "    tfidf_results.append(tfidf_batch)\n",
    "\n",
    "# 희소 행렬 합치기 (결과 병합)\n",
    "final_tfidf_matrix = vstack(tfidf_results)  # 모든 배치 결과를 하나의 희소 행렬로 결합\n",
    "\n",
    "# 결과 저장 (옵션)\n",
    "joblib.dump(final_tfidf_matrix, 'final_tfidf_matrix.pkl')  # 결과를 파일로 저장 (필요한 경우)\n",
    "\n",
    "# 결과 확인\n",
    "print(final_tfidf_matrix.shape)  # 희소 행렬의 형태 출력 (행: 샘플, 열: 단어)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상위 100개 단어:\n",
      "비타민: 4678.347864210655\n",
      "에너지: 4200.570580034271\n",
      "배변: 3854.336257172601\n",
      "혈액: 3812.3215550227915\n",
      "항산화: 3793.3290254096873\n",
      "산소: 3769.231749214203\n",
      "면역: 3766.880869395086\n",
      "유산균: 3254.479111933815\n",
      "감소: 3008.492446754109\n",
      "기억: 2845.5906389369397\n",
      "피로: 2783.032539733685\n",
      "골다공증: 2207.1877303518\n",
      "단백질: 2200.6113133665554\n",
      "칼슘: 2130.345497804262\n",
      "지방: 1930.3185987553309\n",
      "시스테인: 1521.2363199142403\n",
      "피부: 1520.6129753008752\n",
      "체지방: 1513.3719572724335\n",
      "아연: 1455.6845310617175\n",
      "프로바이오틱스: 1431.4937970582157\n",
      "혈행: 1239.0534434972858\n",
      "근육: 1162.8954368526597\n",
      "신경: 1131.9009699257895\n",
      "콜레스테롤: 1083.1251164582902\n",
      "아미노산: 1076.9195921891505\n",
      "관절: 1075.3969392964261\n",
      "지질: 1017.7424233612493\n",
      "연골: 1007.6127497282199\n",
      "중성: 987.1897055728577\n",
      "엽산: 909.2835226819092\n",
      "점막: 806.8639137065445\n",
      "홍삼: 784.4734572421034\n",
      "상피세포: 758.3461828948556\n",
      "치아: 703.5706923638827\n",
      "경관: 604.2722604279988\n",
      "태아: 604.0387515341207\n",
      "운반: 603.1206662895527\n",
      "셀렌: 597.907903679144\n",
      "혈당: 581.2977033796352\n",
      "나이아신: 562.8007889046536\n",
      "식후: 556.6049067229468\n",
      "상승: 550.3032330158466\n",
      "판토텐산: 538.3353640536776\n",
      "보지: 525.993648254662\n",
      "아캄: 524.2457430021632\n",
      "갱년기: 521.963953196391\n",
      "여성: 499.4530502676113\n",
      "영문: 492.43039745430235\n",
      "국문: 491.97091737882863\n",
      "보습: 489.8387337275753\n",
      "노화: 486.65830558796273\n",
      "구성: 431.64344025249943\n",
      "비오틴: 430.7424295447654\n",
      "밀도: 423.3497536149136\n",
      "색소: 423.3497536149136\n",
      "황반: 423.3497536149136\n",
      "마그네슘: 421.41233283170897\n",
      "셀레늄: 411.9813236125481\n",
      "지구력: 394.78503968378476\n",
      "유익: 364.938401935157\n",
      "혈압: 357.2151312236302\n",
      "구강: 356.30250720828906\n",
      "항균: 352.55857555979236\n",
      "스트레스: 347.03588199562375\n",
      "밀크: 331.9884596160898\n",
      "망간: 315.23742399310055\n",
      "마리: 315.1513462636822\n",
      "전립선: 311.77358168457636\n",
      "프로폴리스: 303.09468554566797\n",
      "식이섬유: 290.82183983858715\n",
      "생리: 280.41118995392947\n",
      "장내: 272.5276689607727\n",
      "손상: 260.0653124974734\n",
      "바나바: 249.67396828978804\n",
      "은행: 245.4006459322498\n",
      "코엔자임: 241.72666128300028\n",
      "상태: 240.00681649386098\n",
      "골드: 235.4346213164835\n",
      "자외선: 233.93632103035196\n",
      "완화: 232.78171589841523\n",
      "긴장: 232.3952090929914\n",
      "관여: 221.12606800228528\n",
      "타코: 214.67782836921006\n",
      "녹차: 206.96735888147995\n",
      "엠에스: 197.3461536543065\n",
      "효소: 196.49762462846454\n",
      "올리고당: 195.81409109300054\n",
      "호르몬: 192.55143130513534\n",
      "크롬: 182.8270841430102\n",
      "프락: 174.06445571382372\n",
      "과민반응: 173.80322495143383\n",
      "회복: 168.27048745700083\n",
      "구리: 164.74895311989235\n",
      "팔메: 162.80429878502696\n",
      "히알루론산: 157.33490307837164\n",
      "복합: 149.36152314264692\n",
      "테아닌: 146.5043446955725\n",
      "원료: 138.2561026723503\n",
      "균형: 137.715300597424\n",
      "필수: 136.13494987705326\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "from scipy.sparse import vstack\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "\n",
    "# CSV 파일 로드\n",
    "data = pd.read_csv('../data/supplements_20250311.csv')  # 텍스트 컬럼 '기능성' 사용\n",
    "\n",
    "# KoNLPy의 Okt 객체를 사용하여 형태소 분석\n",
    "okt = Okt()\n",
    "\n",
    "# 불용어 목록을 텍스트 파일에서 읽어오기\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords = file.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "# 불용어 리스트 로드\n",
    "stop_words = load_stopwords('../data/stopwords-ko.txt')\n",
    "\n",
    "# 텍스트 전처리 함수 (형태소 분석 및 불용어 처리)\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):  # NaN 값이 있을 경우 빈 문자열로 처리\n",
    "        return ''\n",
    "    try:\n",
    "        # 형태소 분석을 통해 명사만 추출\n",
    "        nouns = okt.nouns(text)\n",
    "        # 불용어 제거\n",
    "        filtered_nouns = [word for word in nouns if word not in stop_words]\n",
    "        return ' '.join(filtered_nouns) if filtered_nouns else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:30]}... -> {e}\")  # 텍스트 앞 30글자만 출력\n",
    "        return ''\n",
    "\n",
    "# 텍스트 전처리\n",
    "data['processed_text'] = data['기능성'].apply(preprocess_text)\n",
    "\n",
    "# TF-IDF 벡터화 함수\n",
    "def process_tfidf_batch(batch_data, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer()  # 불용어 처리 이미 완료된 데이터 사용\n",
    "        tfidf_matrix = vectorizer.fit_transform(batch_data)  # TF-IDF 매트릭스 생성\n",
    "    else:\n",
    "        tfidf_matrix = vectorizer.transform(batch_data)  # 기존 벡터라이저를 사용하여 변환\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# 배치 단위로 처리하기\n",
    "batch_size = 5000  # 5000개씩 처리 (배치 크기 조정 가능)\n",
    "tfidf_results = []\n",
    "\n",
    "# 첫 번째 배치에서 벡터라이저 학습\n",
    "first_batch = data['processed_text'][:batch_size]\n",
    "tfidf_matrix, tfidf_vectorizer = process_tfidf_batch(first_batch)\n",
    "\n",
    "tfidf_results.append(tfidf_matrix)\n",
    "\n",
    "# 나머지 배치 처리\n",
    "for start in range(batch_size, len(data), batch_size):\n",
    "    end = min(start + batch_size, len(data))\n",
    "    batch_data = data['processed_text'][start:end]\n",
    "    \n",
    "    tfidf_batch, _ = process_tfidf_batch(batch_data, tfidf_vectorizer)\n",
    "    tfidf_results.append(tfidf_batch)\n",
    "\n",
    "# 희소 행렬 합치기 (결과 병합)\n",
    "final_tfidf_matrix = vstack(tfidf_results)  # 모든 배치 결과를 하나의 희소 행렬로 결합\n",
    "\n",
    "# 단어 중요도 합산\n",
    "word_importance = np.array(final_tfidf_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "# 단어와 중요도 (TF-IDF 점수 합) 연결\n",
    "words = list(tfidf_vectorizer.get_feature_names_out())\n",
    "word_importance_dict = dict(zip(words, word_importance))\n",
    "\n",
    "# 중요도가 높은 단어들 상위 100개 추출\n",
    "sorted_words = sorted(word_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "top_100_words = sorted_words[:100]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"상위 100개 단어:\")\n",
    "for word, importance in top_100_words:\n",
    "    print(f\"{word}: {importance}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUPPLE IT-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
